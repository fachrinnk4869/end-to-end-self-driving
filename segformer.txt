model_bb EncoderDecoder(
  (data_preprocessor): SegDataPreProcessor()
  (backbone): MixVisionTransformer(
    (layers): ModuleList(
      (0): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(3, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0-1): 2 x TransformerEncoderLayer(
            (norm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU(approximate='none')
              (layers): Sequential(
                (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                (2): GELU(approximate='none')
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
      )
      (1): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0-1): 2 x TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU(approximate='none')
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU(approximate='none')
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      )
      (2): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0-1): 2 x TransformerEncoderLayer(
            (norm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=160, out_features=160, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU(approximate='none')
              (layers): Sequential(
                (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
                (2): GELU(approximate='none')
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)
      )
      (3): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(160, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0-1): 2 x TransformerEncoderLayer(
            (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU(approximate='none')
              (layers): Sequential(
                (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                (2): GELU(approximate='none')
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (decode_head): SegformerHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (conv_seg): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (fusion_conv): ConvModule(
      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activate): ReLU(inplace=True)
    )
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
